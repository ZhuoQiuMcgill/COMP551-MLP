{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "NUU-6l45-bUq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP class"
      ],
      "metadata": {
        "id": "wPDn8W3jm7bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(self, size, add_bias, activation_function):\n",
        "        self.size = size\n",
        "        self.depth = len(size)\n",
        "        self.weights = []\n",
        "        self.activation_function = activation_function\n",
        "        self.add_bias = add_bias\n",
        "\n",
        "        for i in range(self.depth - 1):\n",
        "            self.weights.append(np.random.normal(scale=0.1, size=(self.size[i], self.size[i + 1])))\n",
        "\n",
        "        if self.add_bias:\n",
        "            self.weights[0] = np.row_stack([self.weights[0], np.ones(self.weights[0].shape[1])])\n",
        "\n",
        "    def predict(self, x):\n",
        "        yh = x\n",
        "        for i in range(self.depth - 2):\n",
        "            yh = self.activation_function[0](np.dot(yh, self.weights[i]))\n",
        "        return MLP.softmax(np.dot(yh, self.weights[-1]))\n",
        "\n",
        "    def fit(self, x_train, y_train, learning_rate=0.1, max_iters=1000,\n",
        "            mini_batch_size=32, epsilon=1e-8, add_regularization=\"None\", lambdaa=0.01):\n",
        "        num_batches = len(x_train) // mini_batch_size\n",
        "\n",
        "        norms = np.array([np.inf])\n",
        "        t = 1\n",
        "        while np.any(norms > epsilon) and t < max_iters:\n",
        "            # Shuffle the training data\n",
        "            indices = np.random.permutation(len(x_train))\n",
        "            x_train = x_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "\n",
        "            # Train on each batch\n",
        "            for batch in range(num_batches):\n",
        "                # Select a batch of data\n",
        "                start = batch * mini_batch_size\n",
        "                end = (batch + 1) * mini_batch_size\n",
        "                x_batch = x_train[start:end]\n",
        "                y_batch = y_train[start:end]\n",
        "                # print(x_batch.shape, y_batch.shape)\n",
        "                grad = self.gradient(x_batch, y_batch, add_regularization, lambdaa)\n",
        "\n",
        "                # Update the weights using stochastic gradient descent\n",
        "                for i in range(len(self.weights)):\n",
        "                    self.weights[i] -= learning_rate * grad[i]\n",
        "\n",
        "                t += 1\n",
        "                norms = np.array([np.linalg.norm(g) for g in grad])\n",
        "\n",
        "    def gradient(self, x, y, add_regularization, lambdaa):\n",
        "        # Forward pass\n",
        "        zs = [x]\n",
        "        z = x\n",
        "        for i in range(self.depth - 2):\n",
        "            z = np.dot(z, self.weights[i])\n",
        "            zs.append(z)\n",
        "            z = self.activation_function[0](z)\n",
        "        z = np.dot(z, self.weights[-1])\n",
        "        zs.append(z)\n",
        "\n",
        "        yh = MLP.softmax(z)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = yh - y\n",
        "\n",
        "        grads = [None] * (self.depth - 1)\n",
        "        for i in range(len(grads) - 1, -1, -1):\n",
        "            grads[i] = np.dot(self.activation_function[0](zs[i]).T, delta) / x.shape[0]\n",
        "            if add_regularization == \"L1\":\n",
        "                grads[i][1:] += lambdaa * np.sign(self.weights[i][1:])\n",
        "            elif add_regularization == \"L2\":\n",
        "                grads[i][1:] += lambdaa * self.weights[i][1:]\n",
        "            delta = np.dot(delta, self.weights[i].T)\n",
        "            delta = delta * self.activation_function[1](self.activation_function[0](zs[i]))\n",
        "\n",
        "        return grads\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_acc(yh, y):\n",
        "        correct = 0\n",
        "        for i in range(len(yh)):\n",
        "            if np.argmax(yh[i]) == np.argmax(y[i]):\n",
        "                correct += 1\n",
        "\n",
        "        return round(correct / len(yh), 3)\n"
      ],
      "metadata": {
        "id": "YHY-Sd1A-cq1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        data = pickle.load(fo, encoding='bytes')\n",
        "    return data\n",
        "\n",
        "\n",
        "def translate(label):\n",
        "    y_train = []\n",
        "    for c in label:\n",
        "        temp = [0 for i in range(10)]\n",
        "        temp[c] = 1\n",
        "        y_train.append(temp)\n",
        "    return np.array(y_train)\n",
        "\n",
        "\n",
        "def training_data():\n",
        "    data = unpickle(\"data_batch_1\")\n",
        "    x1 = data[b'data']\n",
        "    y1 = translate(data[b'labels'])\n",
        "\n",
        "    data = unpickle(\"data_batch_2\")\n",
        "    x2 = data[b'data']\n",
        "    y2 = translate(data[b'labels'])\n",
        "\n",
        "    data = unpickle(\"data_batch_3\")\n",
        "    x3 = data[b'data']\n",
        "    y3 = translate(data[b'labels'])\n",
        "\n",
        "    data = unpickle(\"data_batch_4\")\n",
        "    x4 = data[b'data']\n",
        "    y4 = translate(data[b'labels'])\n",
        "\n",
        "    data = unpickle(\"data_batch_5\")\n",
        "    x5 = data[b'data']\n",
        "    y5 = translate(data[b'labels'])\n",
        "\n",
        "    x_train = x1\n",
        "    x_train = np.vstack((x_train, x2))\n",
        "    x_train = np.vstack((x_train, x3))\n",
        "    x_train = np.vstack((x_train, x4))\n",
        "    x_train = np.vstack((x_train, x5))\n",
        "\n",
        "    y_train = y1\n",
        "    y_train = np.vstack((y_train, y2))\n",
        "    y_train = np.vstack((y_train, y3))\n",
        "    y_train = np.vstack((y_train, y4))\n",
        "    y_train = np.vstack((y_train, y5))\n",
        "\n",
        "    return x_train, y_train\n",
        "\n",
        "\n",
        "def normalize(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Normalize the training and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    X_train (numpy array): The training set of shape `(n_train, d)`.\n",
        "    X_test (numpy array): The test set of shape `(n_test, d)`.\n",
        "\n",
        "    Returns:\n",
        "    X_train_norm (numpy array): The normalized training set.\n",
        "    X_test_norm (numpy array): The normalized test set.\n",
        "    \"\"\"\n",
        "    # Compute mean and standard deviation of the training set\n",
        "    mean = np.mean(X_train, axis=0)\n",
        "    std = np.std(X_train, axis=0)\n",
        "    # print(mean.shape, std.shape)\n",
        "    # Normalize training set and test set\n",
        "    X_train_norm = (X_train - mean) / std\n",
        "    X_test_norm = (X_test - mean) / std\n",
        "\n",
        "    return X_train_norm, X_test_norm"
      ],
      "metadata": {
        "id": "tgyuiIZ5m_Dl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions"
      ],
      "metadata": {
        "id": "EcojAWLsn4HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(int)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "\n",
        "def leaky_relu(x, alpha=0.1):\n",
        "    return np.maximum(alpha * x, x)\n",
        "\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.1):\n",
        "    dx = np.ones_like(x)\n",
        "    dx[x < 0] = alpha\n",
        "    return dx\n",
        "\n",
        "\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def linear_derivative(x):\n",
        "    return np.ones_like(x)"
      ],
      "metadata": {
        "id": "xJGc-mTsnxeE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot functions"
      ],
      "metadata": {
        "id": "JV8AzIEIoMBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search():\n",
        "    lrs = [0.0005, 0.001, 0.003, 0.005, 0.01, 0.03, 0.05]\n",
        "    max_iters = [10000, 50000, 100000]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel(\"Max iterations\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.title(\"Accuracy in different learning rate\")\n",
        "\n",
        "    for lr in lrs:\n",
        "        acc_lr = []\n",
        "        for max_iter in max_iters:\n",
        "            mlp = MLP([3072, 10], ADD_BIAS, [linear, linear_derivative])\n",
        "\n",
        "            start_time = time.time()\n",
        "            mlp.fit(X_TRAIN, Y_TRAIN, learning_rate=lr, max_iters=max_iter, mini_batch_size=32)\n",
        "            end_time = time.time()\n",
        "\n",
        "            y_predict = mlp.predict(X_TEST)\n",
        "            acc = round(mlp.evaluate_acc(y_predict, Y_TEST), 3) * 100\n",
        "            acc_lr.append(acc)\n",
        "            print(\"(\", lr, max_iter, \") done, acc =\", acc, \"%, runtime = \", round(end_time - start_time, 3), \"s\")\n",
        "        plt.plot(max_iters, acc_lr)\n",
        "\n",
        "    plt.legend(lrs)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def regularization():\n",
        "    lambdaas = [0.01, 0.05, 0.1, 0.5, 1]\n",
        "    regularizations = [\"None\", \"L1\", \"L2\"]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel(\"Lambda\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.title(\"Accuracy in different regularization and lambda\")\n",
        "\n",
        "    for re in regularizations:\n",
        "        accs = []\n",
        "        for lambdaa in lambdaas:\n",
        "            mlp = MLP([3072, 256, 256, 10], ADD_BIAS, [relu, relu_derivative])\n",
        "\n",
        "            start_time = time.time()\n",
        "            mlp.fit(X_TRAIN, Y_TRAIN,\n",
        "                    learning_rate=0.005, max_iters=50000, mini_batch_size=32, add_regularization=re, lambdaa=lambdaa)\n",
        "            end_time = time.time()\n",
        "\n",
        "            y_predict = mlp.predict(X_TEST)\n",
        "            acc = round(mlp.evaluate_acc(y_predict, Y_TEST), 3) * 100\n",
        "            accs.append(acc)\n",
        "            print(\"(\", re, lambdaa, \") done, acc =\", acc, \"%, runtime = \", round(end_time - start_time, 3), \"s\")\n",
        "        plt.plot(lambdaas, accs)\n",
        "\n",
        "    plt.legend(regularizations)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def normalized_data():\n",
        "    x_train, y_train = training_data()\n",
        "    x_test = TEST_DATA[b'data']\n",
        "\n",
        "    # Non-normalized data\n",
        "    mlp = MLP([3072, 256, 256, 10], False, [relu, relu_derivative])\n",
        "    mlp.fit(x_train, y_train,\n",
        "            learning_rate=0.005, max_iters=50000, mini_batch_size=32, add_regularization=\"None\")\n",
        "    y_predict = mlp.predict(x_test)\n",
        "    acc_non_normalized = round(mlp.evaluate_acc(y_predict, Y_TEST), 3) * 100\n",
        "\n",
        "    # Normalized data\n",
        "    x_train, x_test = normalize(x_train, x_test)\n",
        "    mlp = MLP([3072, 256, 256, 10], False, [relu, relu_derivative])\n",
        "    mlp.fit(x_train, y_train,\n",
        "            learning_rate=0.005, max_iters=50000, mini_batch_size=32, add_regularization=\"None\")\n",
        "    y_predict = mlp.predict(x_test)\n",
        "    acc_normalized = round(mlp.evaluate_acc(y_predict, Y_TEST), 3) * 100\n",
        "\n",
        "    plt.figure()\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.title(\"Normalized data\")\n",
        "    plt.bar([\"Non-normalized\", \"Normalized\"], [acc_non_normalized, acc_normalized])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def activation_function_selection():\n",
        "    activation_functions_name = [\"ReLU\", \"leaky-ReLU\", \"tanh\"]\n",
        "    activation_functions = [[relu, relu_derivative], [leaky_relu, leaky_relu_derivative], [tanh, tanh_derivative]]\n",
        "\n",
        "    accs = []\n",
        "    mlp = MLP([3072, 256, 256, 10], ADD_BIAS, [])\n",
        "    init_weights = mlp.weights[:]\n",
        "    for func in activation_functions:\n",
        "        mlp.activation_function = func\n",
        "        mlp.weights = init_weights[:]\n",
        "\n",
        "        start_time = time.time()\n",
        "        mlp.fit(X_TRAIN, Y_TRAIN,\n",
        "                learning_rate=0.005, max_iters=50000, mini_batch_size=32, add_regularization=\"None\")\n",
        "        end_time = time.time()\n",
        "\n",
        "        y_predict = mlp.predict(X_TEST)\n",
        "        acc = round(mlp.evaluate_acc(y_predict, Y_TEST), 3) * 100\n",
        "        accs.append(acc)\n",
        "        print(func, \"done, acc =\", acc, \"%, runtime = \", round(end_time - start_time, 3), \"s\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.title(\"Accuracy in different activation function\")\n",
        "    plt.bar(activation_functions_name, accs)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def different_width():\n",
        "    widths = [\"64\", \"128\", \"256\", \"512\", \"1024\"]\n",
        "    accs = []\n",
        "\n",
        "    for width in widths:\n",
        "        mlp = MLP([3072, int(width), 10], ADD_BIAS, [relu, relu_derivative])\n",
        "        start_time = time.time()\n",
        "        mlp.fit(X_TRAIN, Y_TRAIN,\n",
        "                learning_rate=0.001, max_iters=100000, mini_batch_size=32, add_regularization=\"None\")\n",
        "        end_time = time.time()\n",
        "\n",
        "        y_predict = mlp.predict(X_TEST)\n",
        "        acc = round(mlp.evaluate_acc(y_predict, Y_TEST), 3) * 100\n",
        "        accs.append(acc)\n",
        "        print(width, \"done, acc =\", acc, \"%, runtime = \", round(end_time - start_time, 3), \"s\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel(\"Width\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.title(\"Accuracy in different widths\")\n",
        "    plt.plot(widths, accs)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def different_depth():\n",
        "    accs = []\n",
        "\n",
        "    # no hidden layer\n",
        "    mlp = MLP([3072, 10], ADD_BIAS, [linear, linear_derivative])\n",
        "    start_time = time.time()\n",
        "    mlp.fit(X_TRAIN, Y_TRAIN,\n",
        "            learning_rate=0.005, max_iters=50000, mini_batch_size=32, add_regularization=\"None\")\n",
        "    end_time = time.time()\n",
        "    y_predict = mlp.predict(X_TEST)\n",
        "    acc = round(mlp.evaluate_acc(y_predict, Y_TEST), 3) * 100\n",
        "    accs.append(acc)\n",
        "    print(\"Linear done, acc =\", acc, \"%, runtime = \", round(end_time - start_time, 3), \"s\")\n",
        "\n",
        "    # 1 hidden layer\n",
        "    mlp = MLP([3072, 256, 10], ADD_BIAS, [relu, relu_derivative])\n",
        "    start_time = time.time()\n",
        "    mlp.fit(X_TRAIN, Y_TRAIN,\n",
        "            learning_rate=0.001, max_iters=100000, mini_batch_size=32, add_regularization=\"None\")\n",
        "    end_time = time.time()\n",
        "    y_predict = mlp.predict(X_TEST)\n",
        "    acc = round(mlp.evaluate_acc(y_predict, Y_TEST), 3) * 100\n",
        "    accs.append(acc)\n",
        "    print(\"1 hidden done, acc =\", acc, \"%, runtime = \", round(end_time - start_time, 3), \"s\")\n",
        "\n",
        "    # 2 hidden layer\n",
        "    mlp = MLP([3072, 256, 256, 10], ADD_BIAS, [relu, relu_derivative])\n",
        "    start_time = time.time()\n",
        "    mlp.fit(X_TRAIN, Y_TRAIN,\n",
        "            learning_rate=0.005, max_iters=50000, mini_batch_size=32, add_regularization=\"None\")\n",
        "    end_time = time.time()\n",
        "    y_predict = mlp.predict(X_TEST)\n",
        "    acc = round(mlp.evaluate_acc(y_predict, Y_TEST), 3) * 100\n",
        "    accs.append(acc)\n",
        "    print(\"2 hidden done, acc =\", acc, \"%, runtime = \", round(end_time - start_time, 3), \"s\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel(\"Number of hidden layer\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.title(\"Accuracy in different number of hidden layers\")\n",
        "    plt.bar([\"0\", \"1\", \"2\"], accs)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def run():\n",
        "    mlp = MLP([3072, 256, 10], ADD_BIAS, [relu, relu_derivative])\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(\"start time:\", start_time)\n",
        "    mlp.fit(X_TRAIN, Y_TRAIN,\n",
        "            learning_rate=0.005, max_iters=50000, mini_batch_size=32, add_regularization=\"None\", lambdaa=0.01)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(\"end time:\", end_time)\n",
        "\n",
        "    Y_PREDICT = mlp.predict(X_TEST)\n",
        "    print(\"y_predict: \", Y_PREDICT[0])\n",
        "    print(\"y_test: \", Y_TEST[0])\n",
        "    ACC = mlp.evaluate_acc(Y_PREDICT, Y_TEST)\n",
        "    print(\"acc = \", ACC * 100, \"%\\nruntime: \", round(end_time - start_time, 3), \"s\")\n",
        "\n",
        "\n",
        "def evaluate_acc(yh, y):\n",
        "    correct = 0\n",
        "    for i in range(len(yh)):\n",
        "        if np.argmax(yh[i]) == np.argmax(y[i]):\n",
        "            correct += 1\n",
        "    return round(correct / len(yh), 3)\n",
        "\n",
        "\n",
        "def train(eps, lr, dl, net):\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "    for t in range(eps):\n",
        "        for i, data in enumerate(dl, 0):\n",
        "            inputs, labels = data\n",
        "            if torch.cuda.is_available():\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "def train_and_plt_cnn(max_es, l_rates, d_loader):\n",
        "    plt.figure()\n",
        "    plt.xlabel(\"Max Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy in different learning rate in CNN\")\n",
        "\n",
        "    for lr in l_rates:\n",
        "        acc_lr = []\n",
        "        for e in max_es:\n",
        "            start = time.time()\n",
        "            model = CNN(outputs=10)\n",
        "            model.to(device)\n",
        "            train(int(e), lr, d_loader, model)\n",
        "            y_pred = model(x_test_tensor)\n",
        "            y_pred = y_pred.cpu()\n",
        "            y_pred = y_pred.detach().numpy()\n",
        "            accuracy = round(evaluate_acc(y_pred, Y_TEST), 3) * 100\n",
        "            acc_lr.append(accuracy)\n",
        "            end = time.time()\n",
        "            running_time = end - start\n",
        "            print(\"hyperparams: \" + str(e) + ' ' + str(lr))\n",
        "            print(\"Time elapsed: \" + str(running_time))\n",
        "        plt.plot(epochs, acc_lr)\n",
        "\n",
        "    plt.legend(lrs)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_and_plt_res(max_es, l_rates, d_loader):\n",
        "    plt.figure()\n",
        "    plt.xlabel(\"Max Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy in different learning rate in pretrained RESnet\")\n",
        "\n",
        "    for lr in l_rates:\n",
        "        acc_lr = []\n",
        "        for e in max_es:\n",
        "            start = time.time()\n",
        "\n",
        "            resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "\n",
        "            for param in resnet.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            num_features = resnet.fc.in_features\n",
        "            resnet.fc = nn.Sequential(\n",
        "                nn.Linear(num_features, 256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(256, 256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(256, 10),\n",
        "                nn.Softmax(dim=1)\n",
        "            )\n",
        "\n",
        "            resnet.to(device)\n",
        "            train(int(e), lr, d_loader, resnet)\n",
        "            y_pred = resnet(x_test_tensor)\n",
        "            y_pred = y_pred.cpu()\n",
        "            y_pred = y_pred.detach().numpy()\n",
        "            accuracy = round(evaluate_acc(y_pred, Y_TEST), 3) * 100\n",
        "            acc_lr.append(accuracy)\n",
        "            end = time.time()\n",
        "            running_time = end - start\n",
        "            print(\"hyperparams: \" + str(e) + ' ' + str(lr))\n",
        "            print(\"Time elapsed: \" + str(running_time))\n",
        "        plt.plot(epochs, acc_lr)\n",
        "\n",
        "    plt.legend(lrs)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_and_plot_time_diff(d_loader):\n",
        "    plt.figure()\n",
        "    plt.ylabel(\"Training Time\")\n",
        "    plt.title(\"Training Time Difference Between CNN and Pretrained ResNet Model\")\n",
        "\n",
        "    model_names = ['CNN', 'Pretrained ResNet']\n",
        "    training_times = []\n",
        "\n",
        "    cnn = CNN(outputs=10)\n",
        "    cnn.to(device)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    train(50, 0.05, d_loader, cnn)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    cnn_training_time = end - start\n",
        "    training_times.append(cnn_training_time)\n",
        "\n",
        "    resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "    for param in resnet.parameters():\n",
        "        param.requires_grad = False\n",
        "    num_features = resnet.fc.in_features\n",
        "    resnet.fc = nn.Sequential(\n",
        "        nn.Linear(num_features, 256),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(256, 10),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "    resnet.to(device)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    train(50, 0.05, d_loader, resnet)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    resnet_training_time = end - start\n",
        "    training_times.append(resnet_training_time)\n",
        "\n",
        "    plt.bar(model_names, training_times)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PVxGQfzwoEE7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch CNN"
      ],
      "metadata": {
        "id": "H_hFSPpuqVt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, outputs=10, hidden1=256, hidden2=256):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64*8*8, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        self.fc3 = nn.Linear(hidden2, outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.softmax(self.fc3(x), dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "TfqbP5QJq-Z2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main"
      ],
      "metadata": {
        "id": "Re3wKtcDn-iE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbn0lixCNsQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff972ea-c0d6-41b8-82d6-2ec78a42bf6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear done, acc = 36.199999999999996 %, runtime =  97.98 s\n",
            "1 hidden done, acc = 45.300000000000004 %, runtime =  1763.957 s\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    TEST_DATA = unpickle(\"test_batch\")\n",
        "\n",
        "    ADD_BIAS = False\n",
        "    USING_CNN = False\n",
        "    USING_RESNET = False\n",
        "    X_TRAIN, Y_TRAIN = training_data()\n",
        "    X_TEST = TEST_DATA[b'data']\n",
        "    X_TRAIN, X_TEST = normalize(X_TRAIN, X_TEST)\n",
        "\n",
        "    Y_TEST = translate(TEST_DATA[b'labels'])\n",
        "\n",
        "    if ADD_BIAS:\n",
        "        X_TRAIN = np.column_stack([X_TRAIN, np.ones(X_TRAIN.shape[0])])\n",
        "\n",
        "\n",
        "    different_depth()\n",
        "    if USING_CNN:\n",
        "        X_TRAIN = X_TRAIN.reshape(-1, 3, 32, 32)\n",
        "\n",
        "        X_TEST = X_TEST.reshape(-1, 3, 32, 32)\n",
        "\n",
        "        x_train_tensor = Variable(torch.from_numpy(X_TRAIN).float())\n",
        "        y_train_tensor = Variable(torch.from_numpy(Y_TRAIN).float())\n",
        "        x_test_tensor = Variable(torch.from_numpy(X_TEST).float())\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            x_test_tensor = x_test_tensor.cuda()\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(torch.tensor(x_train_tensor), torch.tensor(y_train_tensor))\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "        lrs = [0.03, 0.05]\n",
        "        epochs = ['5', '10']\n",
        "\n",
        "        train_and_plt_cnn(epochs, lrs, loader)\n",
        "    \n",
        "    elif USING_RESNET:\n",
        "      X_TRAIN = X_TRAIN.reshape(-1, 3, 32, 32)\n",
        "\n",
        "      X_TEST = X_TEST.reshape(-1, 3, 32, 32)\n",
        "\n",
        "      x_train_tensor = Variable(torch.from_numpy(X_TRAIN).float())\n",
        "      y_train_tensor = Variable(torch.from_numpy(Y_TRAIN).float())\n",
        "      x_test_tensor = Variable(torch.from_numpy(X_TEST).float())\n",
        "\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "        x_test_tensor = x_test_tensor.cuda()\n",
        "\n",
        "      dataset = torch.utils.data.TensorDataset(torch.tensor(x_train_tensor), torch.tensor(y_train_tensor))\n",
        "\n",
        "      loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "      lrs = [0.01, 0.03, 0.05, 0.07, 0.1]\n",
        "      epochs = ['5', '10', '20', '50']\n",
        "\n",
        "      train_and_plt_res(epochs, lrs, loader)\n",
        "      # train_and_plot_time_diff(loader)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}